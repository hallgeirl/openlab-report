\section{Introduction}
With an increase of storage space requirements exceeding 15 peta bytes per year at CERN, the number of disks currently deployed at CERN is measured in the thousands, which means that data corruption is not something that is unheard of. At the same time, performing integrity check on production servers means sharing resources with the users of the servers, which results in performance hits for both the integrity check and the users of the system. 

The goal of this project was first and foremost to develop a robust integrity checking daemon for the disk servers at CERN with minimal performance impact on current users, but at the same time perform better than the previous attempt on such a daemon, which was unacceptably slow. I developed improved ways of choosing which file to check next, based on which mount point the file is on, as well as improving the way the daemon checks if the system is busy or not. I also implemented threading for increased performance.

In addition to the checksum daemon itself, I also developed a Django web application for monitoring events (i.e. checksum errors) as they happen, allowing the administrators to locate which server or which cluster, or which machine model that causes problems in order to replace faulty hardware etc. The daemon also produces summary statistics for how many files was checked, how many bad checksums were found, and so on; this data is used by a Lemon sensor for integration with the central monitoring system at CERN.

\subsection{Background}
In November 2009, a first version of a checksum daemon called "checksumd" was developed by the DSS group at the IT department at CERN. This daemon is used as a basis for my work, although it's heavily modified. For completeness, I will here give a brief overview of the CASTOR system and Django, both of which are relevant to this project.

\subsubsection{CASTOR Overview}

CASTOR (CERN Advanced STORage manager) is a hierarchical storage management system used at CERN\cite{castor}, designed to be able to handle the enormous amounts of data produced every year (as per 2011, the storage requirements grows by about 15 peta bytes every year). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{gfx/castor_simplified}
\caption{Simplified view of the hierarchial storage system at CERN}
\label{fig:castor}
\end{figure}

There are two kinds of storage media mainly in use at CERN: Magnetic tapes and disks. Tapes are cheap and energy efficient, but slow; disks are more expensive and always uses energy, even when not read, but they are fast. This has been combined at CERN into a hierarchical system where tapes are used as the main media of storage, but disks are used as a cache. When a user requests a file, and the file is not on any of the disks it will be retrieved from the tapes, put on one of the disk servers, and served to the user. If the same file is requested by another user shortly after, it can be fetched directly from disk, eliminating the need to scan the tape drives.

CASTOR provides transparent access to the tape storage by only letting users store and read files to/from the disk cache. Although the fact that there is a tape back-end is not completely transparent due to magnitudes of difference in latencies for fetching files not in the disk cache, no user has to explicitly store data to the tapes as this is handled by the CASTOR Stager. Another important component is the name server, which keeps track of the directory structure and file names.

\paragraph{The need for integrity checking}
The files on the tapes and disks may for various reasons become corrupted. It could happen due to an error while transferring from the tape to the disk, while it's sitting on the disk due to a physical flaw on the disk, and so on. It's important to catch these errors so that if a file stored in one of the disks are getting corrupted, it can be restored from a good tape copy. It's also useful to see if there are patterns of corrupted files, e.g. if there is a specific machine or cluster that has a higher than average error rate due to hardware errors, so that this hardware can be replaced.

CASTOR already stores an Adler-32 checksum for all files. Typically this checksum is stored in extended file attributes, although it can also be stored in the name server. There is already some integrity checking in place, e.g. when a file is transferred from tape to disk; however, there is currently no system running for detecting checksum errors that occurs during the time a file remains on the disk server. With literally thousands of disks, such errors are not unreasonable. 

\subsubsection{Django}
Django is a web application development framework written in Python that aims to handle many of the tedious tasks with web development for you, like URL parsing, session management, database queries and so on.\cite{django} It is based on a model-template-view architecture, where a model is a class with fields mapped to columns in a relational database, the template is the HTML code with support for a special template language used to insert data from views, and the view is the component that retrieves data from the model, and serves it to the template, possibly with additional logic added. The models and views are both written in Python.

\subsubsection{Lemon}
Lemon stands for LHC Era MONitoring and is a system used for monitoring of computing devices at CERN. The system is built up by sensors (which in turn has metrics), agents and a server (or a Central Measurement Repository). Sensors retrieve data using their defined metrics (which can be a number (e.g. number of files in /tmp), a tuple (e.g. statistics from checksumd) or a matrix), and the agent retrieves the data from the sensors and sends it to the server. 

