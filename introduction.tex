\section{Introduction}
With an increase of storage space requirements exceeding 15 peta bytes per year at CERN, the number of disks currently deployed at CERN is measured in the thousands, which means that data corruption is not something that is unheard of. At the same time, performing integrity checking on production servers means sharing resources with the users of the servers, which may result in performance hits for the users of the system. 

The goal of this project was first and foremost to develop a robust integrity checking daemon for the disk servers at CERN with minimal performance impact on current users, while at the same time performing better than the previous attempt on such a daemon, which was unacceptably slow. I developed improved an improved method for choosing which file to check next, based on which mount point the file resides, as well as improving the way the daemon checks if the system is busy or not. I also implemented threading for increased performance. The daemon outputs the checksum errors to the syslog and an error log chosen by the user, and it also produces summary statistics for how many files were checked, how many bad checksums were found, and so on. 

In addition to the checksum daemon itself, I developed a Django web application for monitoring events (i.e. checksum errors) as they happen, allowing the administrators to locate which server or which cluster, or which machine model causes problems in order to replace faulty hardware, etc. As an additional feature, this monitor also parses logs of resolved checksum errors, as well as files lost due to disk and server crashes, etc. A Lemon metric instance was also configured to parse the checksumd summaries.

\subsection{Background}
In November 2009, the first version of a checksum daemon called "checksumd" was developed by the DSS group in the IT department at CERN. This daemon is used as a basis for my work, although it's heavily modified. For completeness, I will here give a brief overview of the CASTOR system and Django, both of which are relevant to this project.

\subsubsection{CASTOR Overview}
CASTOR ({\bf C}ERN {\bf A}dvanced {\bf STOR}age manager) is a hierarchical storage management system used at CERN\cite{castor} designed to handle the enormous amounts of data produced every year (as per 2011, the storage requirements grow by about 15 petabytes every year). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{gfx/castor_simplified}
\caption{Simplified view of the hierarchial storage system at CERN}
\label{fig:castor}
\end{figure}

There are two kinds of storage media mainly in use at CERN: Magnetic tapes and disks. Tapes are cheap and energy efficient, but slow; disks are more expensive and always use energy, even when not in use, but they are fast. This has been combined at CERN into a hierarchical system where tapes are used as the main media of storage, but disks are used as a cache. When a user requests a file, and the file is not on any of the disks it will be retrieved from the tapes, put on one of the disk servers, and served to the user. If the same file is requested by another user shortly after, it can be fetched directly from disk, eliminating the need to scan the tape drives.

CASTOR provides transparent access to the tape storage by only letting users store and read files to and from the disk cache. Although the fact that there is a tape back-end is not completely transparent due to magnitudes of difference in latencies, depending on wether the file is in the disk cache or not, no user has to explicitly store data to the tapes as this is handled by the CASTOR Stager. 

Another important component is the name server, which keeps track of the directory structure and file names. The name server may also contain checksums for the files contained on the disk servers, which may be used as an alternative way of checking the integrity for files that do not have a checksum stored in their extended attributes.

\paragraph{The need for integrity checking}
The files on the tapes and disks may for various reasons become corrupted. It could happen due to an error while transferring from the tape to the disk, while it's sitting on the disk due to a physical flaw on the disk, and so on. It's important to catch these errors so that if a file stored in one of the disks is getting corrupted, it can be restored from a good tape copy. It's also useful to see if there are patterns of corrupted files, e.g. if there is a specific machine or cluster that has a higher than average error rate due to hardware errors, so that this hardware can be replaced.

CASTOR already stores an Adler-32 checksum for all files. Typically this checksum is stored in extended file attributes, although it can also be stored in the name server. There is already some integrity checking in place, e.g. when a file is transferred from tape to disk; however, there is currently no system running for detecting checksum errors that occurs during the time a file remains on the disk server. With literally thousands of disks, such errors may not be uncommon. 

\subsubsection{Django}
Django is a web application development framework written in Python that aims to handle many of the tedious tasks related to web development, such as URL parsing, session management, database queries and so on.\cite{django} It is based on a model-template-view architecture, where a model is a class with fields mapped to columns in a relational database, the template is the HTML code with support for a special template language used to insert data from views, and the view is the component that retrieves data from the model, and serves it to the template, possibly with additional logic added. The models and views are both written in Python.

\subsubsection{Lemon}
Lemon stands for LHC Era MONitoring and is a system used for monitoring computing devices at CERN. The system is built up by sensors (which in turn have metrics), agents and a server (or a Central Measurement Repository). Sensors retrieve data using their defined metrics which can be a single value (e.g. number of files in /tmp), a tuple (e.g. statistics from checksumd) or a matrix. The agent then retrieves the data from the sensors and sends it to the server. 

